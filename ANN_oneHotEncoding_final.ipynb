{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4687db-d579-4c33-9fab-67d1959c72f8",
   "metadata": {},
   "source": [
    "### Diplom thesis\n",
    "- vyzkouset/najit spravnou **architekturu neuronove site** (LSTM 2-3 vnitrni vrstvy, droup out vyzkouset, zmensovani velikosti vrstev, hyperopt, pouzit Keras?)\n",
    "- **optimalizace parametru** neuronove site\n",
    "- provest **uceni neuronove site** (to mozna nechat na konec, provest 20x-30x beh nebo cross validaci -> zprumerovat? at je to pouzitelne ono to vychazi dost ruzne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b8ddff8-204b-47b8-b0bd-5d8f7a334544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras module for building LSTM \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import itertools\n",
    "import pickle\n",
    "from numpy import array\n",
    "import math\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "import codecs\n",
    "from SmilesPE.tokenizer import *\n",
    "from SmilesPE.spe2vec import *\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "import keras.utils as ku \n",
    "from keras import models \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from hyperopt.pyll.base import scope \n",
    "from hyperopt import pyll, hp, Trials, fmin, tpe, STATUS_OK\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1433614-56e7-43ab-b5b7-ab29198a53e8",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d73d8f92-ede3-4e4d-a027-10514aaa21fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7159, 11)\n",
      "(7044, 11)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/VEGF2_ChEMBL28-10980_pic50_noDuplicate_canSmiles_sln_deepSmiles_selfies_inchi.csv',sep=',')\n",
    "\n",
    "linearNotation_names = ['canonical_smiles','sln','deep_smiles','selfies','inchi']\n",
    "for notName in linearNotation_names:\n",
    "    df[notName+'_length'] = df[notName].str.len()\n",
    "print(df.shape)\n",
    "# canonical_smiles_length\n",
    "dfClean = df.loc[(df['canonical_smiles_length'] >30) & (df['canonical_smiles_length']<80)]\n",
    "print(dfClean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dddc13f-a4ee-4c58-b047-7f49ce1677cd",
   "metadata": {},
   "source": [
    "#### Pomocne funkce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a25c6f22-30b5-4bde-8a00-07b465827622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharAtBeginningEnd(temp):\n",
    "    lst=[]\n",
    "    lst.append('^')\n",
    "    for l in temp:\n",
    "        lst.append(l)\n",
    "    lst.append('$')\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6b2a9a0-ccbe-45bc-b903-c4cd9dfad496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByAtomwiseTokenizer(chem_notation):\n",
    "    lst=[]\n",
    "    temp = atomwise_tokenizer(chem_notation)\n",
    "    lst.append('^')\n",
    "    for l in temp:\n",
    "        lst.append(l)\n",
    "    lst.append('$')\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c6edea4f-bcfe-4b4f-b1aa-1ae323aa4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBeginningInChI(chem_notation,substring):\n",
    "    res = chem_notation.replace(substring,'')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10095c4d-773f-4e08-9c1c-8838940b90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizationBPEChemLinearNotaion(col_name,df):\n",
    "    fileName = 'data_for_tokenizer/tokenizer-wiki_'+str(col_name)+'.json' \n",
    "    fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=fileName)\n",
    "    return df[col_name].apply(lambda chem_notaion: fast_tokenizer.tokenize(str(chem_notaion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2f62798-fff2-4d53-91bc-182023eefea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharacterStartEndSequenceSplitByCharacters(chem_notation):\n",
    "    chem_notation = \"^\" + chem_notation + \"$\"\n",
    "    lst = [c for c in chem_notation]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e015f2e3-18f4-444c-aa6f-814dbeeff6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeChemNotation(chem_notation,col_name): \n",
    "    spe_vob= codecs.open('SPE_ChEMBL_'+str(col_name)+'.txt')\n",
    "    spe = SPE_Tokenizer(spe_vob)\n",
    "    lst = spe.tokenize(chem_notation)\n",
    "    lst = lst.split(' ')\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8730789f-ac38-4810-8d6a-0fa088210dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByKmerTokenizer(chem_notation):\n",
    "    lst=[]\n",
    "    temp = kmer_tokenizer(chem_notation, ngram=4, stride=1, remove_last = False, exclusive_tokens = None)\n",
    "    lst.append('^')\n",
    "    for l in temp:\n",
    "        lst.append(l)\n",
    "    lst.append('$')\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1efad10b-6d14-4ffe-bb52-f6e3b8f30025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vytvorit list token id + padding \n",
    "def createListTokensId(chem_notation, vocabulary):\n",
    "    # chem_notation je list tokenu / uz je pocatecni a koncovy znak ^, $\n",
    "    token_id_list=[]\n",
    "    for c in chem_notation:\n",
    "        if(c in vocabulary):\n",
    "            token_id_list.append(vocabulary[c])\n",
    "    return token_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1347c34c-5c0d-4176-a699-4b3188161bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutVecTokens(x,max_length):\n",
    "    if(max_length >= len(x)):\n",
    "        return x\n",
    "    x = x[:max_length-2]\n",
    "    x.append('$')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9ee8b8e7-26bf-4a0c-b108-42615c7793df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kontrola, rozdeleni data tak je vzdy stejne i kdyz pouziji data pro jiny chem. lin. zapis\n",
    "def splitData(list_token, ys, RANDOM_STATE):  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(list_token, ys, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE) # 0.25 x 0.8 = 0.2\n",
    "    return X_train,y_train,X_val,y_val,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2dd53ae6-3afa-4ee2-b2f4-5a857b6a1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vytvoreni modelu s tunnenim parametru\n",
    "def create_model(units1,layers,units2,units3,l_rate, X_train):\n",
    "    opt = keras.optimizers.Adam(learning_rate=l_rate)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=units1, input_shape=(X_train.shape[1],X_train.shape[2]))))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    # Middle layers return sequences\n",
    "    for i in range(layers-2):\n",
    "        model.add(Dense(units=units2))\n",
    "        model.add(Dropout(rate=0.5))\n",
    "\n",
    "    # Last layer doesn't return anything\n",
    "    model.add(Dense(units=units3))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=opt ,metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3710ffae-b630-4201-902f-360610ff9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModelLossPlot(result,chem_notation,modelLossPlot_fileName,tokenization):\n",
    "    # plot training history\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(result.history['loss'])\n",
    "    plt.plot(result.history['val_loss'])\n",
    "    plt.title('model loss - '+tokenization+' '+chem_notation)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.savefig(modelLossPlot_fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "660e9384-8c21-4d9f-9b70-b3d27c88b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredictReal(predictions,y_test,tokenization,chem_notation,predictionsPlot_fileName):\n",
    "    # plotPredictReal(y_pred, y_test,tokenization,chem_notation,predictionsPlot_fileName)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.scatter(y_test, predictions, c='crimson')\n",
    "        p1 = max(max( predictions), max(y_test))\n",
    "        p2 = min(min( predictions), min(y_test))\n",
    "        plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "        plt.title(tokenization+' '+chem_notation, fontsize=15)\n",
    "        plt.xlabel('True value', fontsize=15)\n",
    "        plt.ylabel('Prediction', fontsize=15)\n",
    "        plt.axis('equal')\n",
    "        plt.show\n",
    "        plt.savefig(predictionsPlot_fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b270adb3-9aa1-467d-b29c-67a923843b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective funkce pro hyperopt s cross validaci\n",
    "def objective(params, X_train, y_train, n_patience, idx, cut_length,RANDOM_STATE):\n",
    "    es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=n_patience)\n",
    "    \n",
    "    model_nn = KerasRegressor(build_fn = create_model,**params, X_train=X_train, epochs = 400, verbose = 0)\n",
    "    \n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(model_nn, X_train, y_train,scoring='neg_mean_squared_error', cv=kfold,fit_params={'callbacks':[es]},error_score='raise')\n",
    "    score = scores.mean()\n",
    "    std = scores.std()\n",
    "    loss = -score\n",
    "    print('params:\\t',params)\n",
    "    print('score:\\t',loss)\n",
    "    print('std:\\t',std)\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'params': params,\n",
    "        'std' : std,\n",
    "        'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "920a679d-3a93-4e07-bc39-3ea98bfa8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vytvoreni modelu s tunnenim parametru\n",
    "def builtModelBestParams(best_params,cut_length,X_train):\n",
    "    opt = keras.optimizers.Adam(learning_rate=params['l_rate'])\n",
    "    model = Sequential()\n",
    "    # First layer specifies input_shape and returns sequences\n",
    "    model.add(Bidirectional(LSTM(units=params['units1'], input_shape=(X_train.shape[1],X_train.shape[2]))))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    # Middle layers return sequences\n",
    "    for i in range(params['layers']-2):\n",
    "        model.add(Dense(units=params['units2']))\n",
    "        model.add(Dropout(rate=0.5))\n",
    "\n",
    "    # Last layer doesn't return anything\n",
    "    model.add(Dense(units=params['units3']))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=opt ,metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8030442b-6ab9-49b0-8519-0570ec3949a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builtModelWithBestParams(best_params,cut_length,cut_data,j,bestModel_fileName,modelHistory_fileName, bestParams_fileName,predictions_fileName,modelLossPlot_fileName,predictionsPlot_fileName,n_patience,encoding,chem_notation,tokenization,X_train,y_train,X_val,y_val,X_test,y_test):\n",
    "    model = builtModelBestParams(best_params,cut_length,X_train)\n",
    "    mc = ModelCheckpoint(bestModel_fileName, monitor='val_loss', mode='min')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=n_patience)\n",
    "    \n",
    "    # train model\n",
    "    result = model.fit(X_train, y_train, validation_data=(X_val, y_val),  epochs=400,verbose=0, callbacks=[es,mc])\n",
    "    # store information\n",
    "    # ----------------------------------------- BEST MODEL ---------------------------------------\n",
    "    best_model = models.load_model(bestModel_fileName)\n",
    "    # model prediction\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    results_eval = best_model.evaluate(X_val, y_val)\n",
    "    print('\\nn_patience:  ', n_patience)\n",
    "    print('max_length:  ',cut_length)\n",
    "    print('results:',results_eval ,end='\\n')\n",
    "    plotPredictReal(y_pred, y_test,tokenization,chem_notation,predictionsPlot_fileName)\n",
    "    saveModelLossPlot(result,chem_notation,modelLossPlot_fileName,tokenization)\n",
    "    \n",
    "    # TADY S TIM NECO PROVEST - jetli pouziji tak do definice funkce pridat\n",
    "    #store information\n",
    "    # ----------------------------------------- HISTORY ---------------------------------------\n",
    "    with open(modelHistory_fileName, 'w') as f:\n",
    "        np.save(modelHistory_fileName,result.history)\n",
    "    # ----------------------------------------- BEST PARAMS ---------------------------------------\n",
    "    a_file = open(bestParams_fileName, \"wb\")\n",
    "    pickle.dump(best_params, a_file)\n",
    "    a_file.close()\n",
    "    # ----------------------------------------- PREDICTIONS ---------------------------------------\n",
    "    prediction = pd.DataFrame(y_pred, columns=['predictions']).to_csv(predictions_fileName)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return results_eval,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e79c574a-a69c-448c-8374-33679d756455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeOneHotEncoding(df,linearNotation_name,tokenization,lengths_lst_tokenIdxs,j):\n",
    "    substring = 'InChI=1S/'\n",
    "    df['inchi'] = df['inchi'].apply(lambda chem_notation: removeBeginningInChI(chem_notation,substring))\n",
    "    col_name = linearNotation_name\n",
    "    token_col_name = col_name+'_tokens'\n",
    "    tokenIndex_col_name = col_name+'_tokenIndexList'\n",
    "\n",
    "    if(tokenization == 'atom'):\n",
    "        df[token_col_name] = df[col_name].apply(lambda chem_notation: splitByAtomwiseTokenizer(chem_notation))\n",
    "    elif(tokenization == 'bpe'):\n",
    "        df[token_col_name] = tokenizationBPEChemLinearNotaion(col_name,df)\n",
    "        df[token_col_name] = df[token_col_name].apply(lambda lst_tokens: addCharAtBeginningEnd(lst_tokens))\n",
    "    elif(tokenization == 'char'):\n",
    "        df[token_col_name] = df[col_name].apply(lambda chem_notation: addCharacterStartEndSequenceSplitByCharacters(chem_notation))\n",
    "    elif(tokenization == 'spe'):\n",
    "        df[token_col_name] = df[col_name].apply(lambda chem_notation: tokenizeChemNotation(chem_notation,col_name))\n",
    "    elif(tokenization == 'kmer'):\n",
    "        df[token_col_name] = df[col_name].apply(lambda chem_notation: splitByKmerTokenizer(chem_notation))\n",
    "\n",
    "    max_length = lengths_lst_tokenIdxs[tokenization][j]\n",
    "    df[token_col_name] = df[token_col_name].apply(lambda x: cutVecTokens(x,max_length))\n",
    "    CHEM_NOTATION_CHARS = set(df[token_col_name].apply(list).sum())\n",
    "    print(max_length)\n",
    "    print(len(CHEM_NOTATION_CHARS))\n",
    "    \n",
    "    if(len(CHEM_NOTATION_CHARS) * max_length > 100000):\n",
    "        tempN = ((len(CHEM_NOTATION_CHARS) * max_length)-100000)/max_length\n",
    "        n = math.floor(len(CHEM_NOTATION_CHARS)-tempN)\n",
    "        n = len(CHEM_NOTATION_CHARS)-n\n",
    "        print('n =',n)\n",
    "        \n",
    "        temp = df[token_col_name].tolist()\n",
    "        flat_list = [item for sublist in temp for item in sublist]\n",
    "        count_flatList = Counter(flat_list)\n",
    "        keysSort = [pair[0] for pair in sorted(count_flatList.items(), key=lambda item: item[1],reverse=False)]\n",
    "        \n",
    "        print(n)\n",
    "        for i in range(n):\n",
    "            del count_flatList[keysSort[i]]\n",
    "        CHEM_NOTATION_CHARS = count_flatList.keys()\n",
    "        print('nova delka: ',len(CHEM_NOTATION_CHARS))\n",
    "    \n",
    "    vocabulary = {c: i+2 for i,c in enumerate(set(CHEM_NOTATION_CHARS))}\n",
    "    df[tokenIndex_col_name] = df[token_col_name].apply(lambda chem_notation: createListTokensId(chem_notation, vocabulary))\n",
    "    \n",
    "    res = df[tokenIndex_col_name].apply(lambda tokenIndex: tokenIndex).tolist()\n",
    "    vector_with_padding = pad_sequences(res, maxlen=max_length, padding='post', truncating='post')\n",
    "    df[tokenIndex_col_name] = [vector_with_padding[i] for i in range(len(vector_with_padding))]\n",
    "    \n",
    "    data = array(df[tokenIndex_col_name].tolist())\n",
    "    oneHotEncoding = to_categorical(data)\n",
    "    \n",
    "    return np.array(oneHotEncoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d612f1-7aaa-4ca8-9f24-6f7275ef9877",
   "metadata": {},
   "source": [
    "# Trenovani neuronove site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb157aa-5cd3-4c0d-9035-592460beb1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "atom\n",
      "\n",
      "inchi\n",
      "cut_length  160\n",
      "nUnits [128]\n",
      "unitsLst\n",
      " [16, 32, 64, 128]\n",
      "160\n",
      "33\n",
      "(7044, 160, 35)\n",
      "Epoch 00007: early stopping                          \n",
      "Epoch 00008: early stopping                          \n",
      "Epoch 00012: early stopping                          \n",
      "Epoch 00008: early stopping                          \n",
      "Epoch 00010: early stopping                          \n",
      "Epoch 00009: early stopping                          \n",
      "Epoch 00010: early stopping                          \n",
      "  0%|          | 0/1 [30:29<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "# budu delat postupne po tokenizacich, abych to mohla kontrolovat\n",
    "#DEFINIICE PROMENNYCH\n",
    "tokenization_names = ['atom','bpe','char','spe','kmer']\n",
    "linearNotation_names = ['canonical_smiles','sln','deep_smiles','selfies','inchi']\n",
    "encoding = 'oneHot'\n",
    "m_e=1\n",
    "n_patience = 1\n",
    "RANDOM_STATE=42\n",
    "\n",
    "df_score = {'tokenization': ['empty'], 'chem_notation': ['empty'],'score_cross_val':['empty'],'std_cross_val':['empty'],'loss_mse':['empty'],'metric_mse':['empty'],'r2':['empty']}\n",
    "df_score = pd.DataFrame(df_score)\n",
    "\n",
    "fileNameScore = '../data/final_results/'+encoding+'/scoreCrossVal.csv'\n",
    "# kontrola, jestli soubor neexistuje, kdyz poustim znova\n",
    "if os.path.exists(fileNameScore):\n",
    "    os.remove(fileNameScore)\n",
    "df_score.to_csv(fileNameScore,sep=',',header=True,index=False)\n",
    "\n",
    "lengths_lst_tokenIdxs = {   'atom':[60,90,65,62,160],\n",
    "                            'char':[62,105,70,300,160],\n",
    "                            'kmer':[60,92,65,60,160],\n",
    "                            'bpe':[30,62,24,125,90],\n",
    "                            'spe':[13,8,7,31,43]}\n",
    "or_unitsLst = [16, 32, 64, 128]\n",
    "ys = np.array(dfClean['value'])#[:limit]\n",
    "\n",
    "# ITERATE OVER TOKENIZATION\n",
    "for i in range(len(tokenization_names)):\n",
    "    tokenization = tokenization_names[i]\n",
    "    print('-----------------------------------------------------------------------------\\n'+tokenization)\n",
    "    df_score['tokenization'][0] = tokenization\n",
    "    # if(not(tokenization =='kmer')):\n",
    "    #         continue\n",
    "    # ITERATE OVER LINEAR CHEMICAL NOTATION\n",
    "    difSmallNum=5000\n",
    "    for j in range(len(linearNotation_names)):\n",
    "        chem_notation = linearNotation_names[j]\n",
    "\n",
    "        print('\\n'+chem_notation)\n",
    "        df_score['chem_notation'][0] = chem_notation\n",
    "        cut_length = lengths_lst_tokenIdxs[tokenization][j]\n",
    "        \n",
    "        # vybrani poctu neuronu pro vstupni vrstvu\n",
    "        for u in range(len(or_unitsLst)):\n",
    "            if(or_unitsLst[u]>cut_length):\n",
    "                difGreatNum = abs(or_unitsLst[u] - cut_length)\n",
    "                if(u!=0):\n",
    "                    difSmallNum = abs(or_unitsLst[u-1] - cut_length)\n",
    "                if(difGreatNum <= difSmallNum):    \n",
    "                    nUnits = [or_unitsLst[u]]\n",
    "                    unitsLst = or_unitsLst[:(u+1)]\n",
    "                    break\n",
    "                else:\n",
    "                    nUnits = [or_unitsLst[u-1]]\n",
    "                    unitsLst = or_unitsLst[:(u)]\n",
    "                    break\n",
    "            elif(or_unitsLst[u]==cut_length or u==(len(or_unitsLst)-1)):\n",
    "                nUnits = [or_unitsLst[u]]\n",
    "                unitsLst = or_unitsLst[:(u+1)]\n",
    "                break\n",
    "                 \n",
    "        \n",
    "        print('cut_length ',cut_length)\n",
    "        print('nUnits',nUnits)\n",
    "        print('unitsLst\\n',unitsLst)\n",
    "              \n",
    "        space = {'units1'     : hp.choice('units1', nUnits),\n",
    "         'units2'             : hp.choice('units2', unitsLst),\n",
    "         'units3'             : hp.choice('units3', unitsLst),\n",
    "         'layers'             : scope.int(hp.quniform('layers',2,3,1)),\n",
    "         'l_rate'             : hp.loguniform('l_rate', np.log(0.01), np.log(0.2))\n",
    "        }\n",
    "        \n",
    "        data = tokenizeOneHotEncoding(dfClean.copy(),chem_notation,tokenization, lengths_lst_tokenIdxs,j)\n",
    "\n",
    "        X_train,y_train,X_val,y_val,X_test,y_test = splitData(data, ys, RANDOM_STATE)  \n",
    "        \n",
    "        # definice nazvu pro ukladani\n",
    "        startName = '../data/final_results/'+encoding+'/'+tokenization+'/'\n",
    "        bestModel_fileName = startName+'bestModels/'+chem_notation+'.h5'\n",
    "        modelHistory_fileName = startName+'history/'+chem_notation+'.npy'\n",
    "        modelLossPlot_fileName = startName+'lossPlot/'+chem_notation+'.png'\n",
    "        bestParams_fileName = startName+'bestParams/'+chem_notation+'.pkl'\n",
    "        predictions_fileName = startName+'predictions/'+chem_notation+'.csv'\n",
    "        predictionsPlot_fileName = startName+'predictionsPlot/'+chem_notation+'.png'\n",
    "        \n",
    "        # zatim mam jen chemicke linearni zapisy\n",
    "        fmin_objective = partial(objective, n_patience = n_patience, idx=j ,cut_length = cut_length,X_train=X_train,y_train=y_train,RANDOM_STATE=RANDOM_STATE)\n",
    "        \n",
    "        bayes_trials = Trials()\n",
    "        best = fmin(fn = fmin_objective, space = space, algo = tpe.suggest, max_evals = m_e, trials = bayes_trials)#, rstate = np.random.RandomState(50))\n",
    "\n",
    "        best_loss = bayes_trials.results[np.argmin([r['loss'] for r in bayes_trials.results])]['loss']\n",
    "        best_std = bayes_trials.results[np.argmin([r['loss'] for r in bayes_trials.results])]['std']\n",
    "        best_params = bayes_trials.results[np.argmin([r['loss'] for r in bayes_trials.results])]['params']\n",
    "        print('best params:\\n',best_params,'\\nbest loss:\\n',best_loss,'\\nbest std:\\n',best_std,end = '\\n')\n",
    "    \n",
    "        df_score['score_cross_val'][0] = best_loss\n",
    "        df_score['std_cross_val'][0] = best_std\n",
    "        \n",
    "        # natrenovani jednoho modelu s nejlepsimy parametry\n",
    "        results_eval,r2 = builtModelWithBestParams(best_params,cut_length,data,j,bestModel_fileName,modelHistory_fileName, bestParams_fileName,\n",
    "                                 predictions_fileName,modelLossPlot_fileName,predictionsPlot_fileName,n_patience,encoding,chem_notation,\n",
    "                                 tokenization,X_train,y_train,X_val,y_val,X_test,y_test)\n",
    "        \n",
    "        df_score['loss_mse'][0] = results_eval[0]\n",
    "        df_score['metric_mse'][0] = results_eval[1]\n",
    "        df_score['r2'][0] = r2\n",
    "        df_score.to_csv(fileNameScore, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0e541-2db0-413c-a742-8fa249c2f598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2efa61-97be-41ab-b118-e56c66210355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37738cd9-5678-4ff4-8716-04b02f1b2e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
